[Prompt 1]
Look at the encoder/decoder used in jepa_world_model_trainer.

The inputs are 224x224 images.  How can I use techniques from ConvNext and ConvNextV2 to improve the encoder/decoder such that it still has a similar number of parameters, but better pays attention to small (1-2px) movements for small objects (7-10px)?

Give me ideas and an outline before coding.

[Changes]:
* Added a ConvNeXt/ConvNeXtV2-inspired design plan outlining encoder/decoder goals and steps in `jepa_world_model/convnext_encoder_decoder_plan.md`.

[Prompt 2]
Now implement the plan, using the ConvNext2 version.

[Changes]:
* Implemented ConvNeXtV2-style encoder/decoder blocks with GRN, anti-aliased stems, and gated skip fusion in `jepa_world_model/convnextv2.py`.
* Wired the JEPA trainer to emit and consume detail-skip maps for sharper reconstructions when using the ConvNeXtV2 components.

[Prompt 3]
Rebase onto the latest "simplify-jepa" branch

[Changes]:
* Restored a final ConvNeXtV2 upsample stage and refreshed decoder shape/drop-path handling to return full-resolution outputs.
* Streamlined trainer decoder aliasing while keeping the sharpened head aligned with the final output.

[Prompt 4]
Instead of deleting the current encoder/decoder, move it to a separate file, like was done for convnextv2.py.

[Changes]:
* Preserved the original convolutional encoder/decoder in `jepa_world_model/conv_encoder_decoder.py` for reuse alongside ConvNeXtV2.
* Exposed legacy encoder/decoder aliases in `jepa_world_model_trainer.py` so both baselines and ConvNeXtV2 stay selectable.

[Prompt 5]
Please address any inline comments on the diff, as well as any additional instructions below.

Give me an .agent/changes file that includes everything we discussed.

[Changes]:
* Captured the full prompt history and corresponding modifications in this consolidated `.agent/changes` entry.
